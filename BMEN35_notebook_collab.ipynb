{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErikHartman/BMEN35/blob/2023/BMEN35_notebook_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5__sM-AKzFI6"
      },
      "source": [
        "# Exercise about xAI and omics\n",
        "\n",
        "This exercise is about methods in the field of explainable AI (xAI) and they're application to omics data. Specifically, we'll use SHapley Additive exPlanations (SHAP) to introspect trained models to understand their reasoning during classification tasks.\n",
        "\n",
        "The exercise has two parts:\n",
        "\n",
        "1. In the first part, you'll get to train a neural network on the MNIST dataset (digits 0-9). Thereafter you'll explain a set of predictions using SHAP. \n",
        "2. In the second part, we'll apply a similar methodology but to a clinical proteomic dataset. Here, SHAP is used to explain what proteins a classifier deems important when classifying two different subtypes of septic acute kidney injury.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we2durP96qaf",
        "outputId": "c97cfecd-c237-4925-d81e-9d86470524a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.43.0 slicer-0.0.7\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install shap\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here you will train a convolutional neural network to classify the digits 0-9 (MINST dataset). Thereafter you will interpret the network using SHAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBngwm_3_uOS",
        "outputId": "cb9b237a-69a3-4faa-d1cc-1e44db789f13"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "num_classes = 10 # 0-9\n",
        "input_shape = (28, 28, 1) # pixels\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1\n",
        ")\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model is trained, time for interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "OS6b_ogEAI0y",
        "outputId": "063e39c1-b569-4818-b974-6aef637c2c93"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# select a set of background examples to take an expectation over\n",
        "background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n",
        "\n",
        "# Create the explainer object\n",
        "e = shap.DeepExplainer(model, background)\n",
        "\n",
        "# Compute the SHAP values\n",
        "shap_values = e.shap_values(x_test[1:5])\n",
        "# Show them in an image\n",
        "shap.image_plot(shap_values, -x_test[1:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Question: what features seem important for the different digits? You can change the x_test slice to see other digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2\n",
        "\n",
        "Now it's time to apply the same principles but to some biological data.\n",
        "\n",
        "The data used here is the septic AKI data (see lecture)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u7NPFb5b1a73"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the data from GitHub\n",
        "data_matrix = pd.read_csv('https://raw.githubusercontent.com/ErikHartman/BMEN35/2023/data/quant_matrix.csv')\n",
        "design_matrix = pd.read_csv('https://raw.githubusercontent.com/ErikHartman/BMEN35/2023/data/inner_design_matrix.tsv', sep=\"\\t\")\n",
        "human_proteome = pd.read_csv('https://raw.githubusercontent.com/ErikHartman/BMEN35/2023/data/human_proteome.gz', sep=\",\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9Z4zII2xF5bX"
      },
      "outputs": [],
      "source": [
        "def get_proteins_triv_name(proteins, proteome):\n",
        "  # function to map Uniprot IDs to more common names.\n",
        "  proteome['accession'] = proteome['accession'].apply(lambda x: x.split('_')[0])\n",
        "  names = []\n",
        "  for protein in proteins:\n",
        "      if protein in proteome['accession'].values:\n",
        "          m = proteome.loc[proteome['accession'] == protein]['trivname'].values\n",
        "          assert len(m) == 1\n",
        "          m = m[0].split('_')[0]\n",
        "      else:\n",
        "          m = protein\n",
        "      names.append(m)\n",
        "  return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "C4k9rQ7v6ud2",
        "outputId": "01e97836-3f75-4854-b363-47e8389770b4"
      },
      "outputs": [],
      "source": [
        "group_one_samples = design_matrix[design_matrix['group'] == 1]['sample'].values\n",
        "group_two_samples = design_matrix[design_matrix['group'] == 2]['sample'].values\n",
        "data_matrix.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocess the data by scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vsP_id_DAaUk",
        "outputId": "5174aa88-8608-498a-a536-3eef545b19f9"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "protein_labels = get_proteins_triv_name(data_matrix['Protein'].values, human_proteome) # the UniProt IDs are translated to more common names.\n",
        "\n",
        "df1 = data_matrix[group_one_samples].T\n",
        "df2 = data_matrix[group_two_samples].T\n",
        "df1.columns = protein_labels\n",
        "df2.columns = protein_labels\n",
        "\n",
        "y = np.array([0 for x in group_one_samples] + [1 for x in group_two_samples]) # this is our Y\n",
        "df_X = pd.concat([df1,df2]).fillna(0)\n",
        "X = df_X.to_numpy()\n",
        "scaler = preprocessing.StandardScaler().fit(X) # makes mean = 0 and variance = 1\n",
        "X_scaled = scaler.transform(X) # this is our scaled X\n",
        "\n",
        "df_X_scaled = pd.DataFrame(X_scaled) # puts everything into a pandas DataFrame\n",
        "df_X_scaled.columns = df_X.columns\n",
        "df_X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "MH-tBvF58BFR",
        "outputId": "b076508a-347c-49f3-d597-6210791a8123"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Train and test how accuarge the XGBoost model is on the data here.\\nImport the necessary packages. Sci-kit learn is available by default in Google Collab\\nso there is no need to !pip install if you're using that package. \""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from xgboost.sklearn import XGBClassifier\n",
        "\"\"\" Train and test how accuarge the XGBoost model is on the data here.\n",
        "Import the necessary packages. Sci-kit learn is available by default in Google Collab\n",
        "so there is no need to !pip install if you're using that package. \"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "zppyD2jIDXU5",
        "outputId": "eeb50d0d-df69-489e-ccf2-4c1bf8fc361f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Create some SHAP plots here! Feel free to stylize them and do whatever you'd like\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" Create some SHAP plots here! Feel free to stylize them and do whatever you'd like\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last task is to look at your top scoring proteins. Can you find any reference linking these to septic AKI? Write in markdown below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
